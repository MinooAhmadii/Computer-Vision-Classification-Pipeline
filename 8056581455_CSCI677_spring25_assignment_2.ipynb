{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u93YJyjNB9WK"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this assignment you will practice putting together a simple image classification pipeline with both non-parametric and parametric methods.\n",
        "\n",
        "In paticular, we will work with the k-Nearest Neighbor, the SVM classifier and the 2-Layered Neural Network for [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. The goals of this assignment are as follows:\n",
        "\n",
        "\n",
        "\n",
        "*   Understand the basic Image Classification pipeline and the data-driven approach (train/predict stages).\n",
        "*   Understand the train/val/test splits and the use of validation data for hyperparameter tuning.\n",
        "*   Implement and apply a Weighted k-Nearest Neighbor (kNN) classifier.\n",
        "*   Implement and apply a Multiclass Support Vector Machine (SVM) classifier.\n",
        "*   Implement and apply a 2-layered Neural Network.\n",
        "*   Understand the differences and tradeoffs between these classifiers.\n",
        "\n",
        "Please fill in all the **TODO** code blocks. Once you are ready to submit:\n",
        "\n",
        "* Export the notebook `CSCI677_spring25_assignment_2.ipynb` as a PDF `[Your USC ID]_CSCI677_spring25_assignment_2.pdf`\n",
        "* Submit your PDF file through Brightspace.\n",
        "\n",
        "Please make sure that the notebook have been run before exporting PDF, and your code and all cell outputs are visible in the your submitted PDF. Regrading request will not be accepted if your code/output is not visible in the original submission. Thank you!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_7K_0sWz0OZ"
      },
      "source": [
        "# **Data Preparation**\n",
        "\n",
        "[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) is a well known dataset composed of 60,000 colored 32x32 images. The utility function `cifar10()` returns the entire CIFAR-10 dataset as a set of four Torch tensors:\n",
        "* `x_train` contains all training images (real numbers in the range  [0,1] )\n",
        "* `y_train` contains all training labels (integers in the range  [0,9] )\n",
        "* `x_test` contains all test images\n",
        "* `y_test` contains all test labels\n",
        "\n",
        "This function automatically downloads the CIFAR-10 dataset the first time you run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Bcwh04DvuTDr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision.datasets import CIFAR10\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "CVP8si9RpSdc"
      },
      "outputs": [],
      "source": [
        "def _extract_tensors(dset, num=None):\n",
        "    x = torch.tensor(dset.data, dtype=torch.float32).permute(0, 3, 1, 2).div_(255)\n",
        "    y = torch.tensor(dset.targets, dtype=torch.int64)\n",
        "    if num is not None:\n",
        "        if num <= 0 or num > x.shape[0]:\n",
        "          raise ValueError('Invalid value num=%d; must be in the range [0, %d]'\n",
        "                          % (num, x.shape[0]))\n",
        "        x = x[:num].clone()\n",
        "        y = y[:num].clone()\n",
        "    return x, y\n",
        "\n",
        "def cifar10(num_train=None, num_test=None):\n",
        "    download = not os.path.isdir('cifar-10-batches-py')\n",
        "    dset_train = CIFAR10(root='.', download=download, train=True)\n",
        "    dset_test = CIFAR10(root='.', train=False)\n",
        "    x_train, y_train = _extract_tensors(dset_train, num_train)\n",
        "    x_test, y_test = _extract_tensors(dset_test, num_test)\n",
        "\n",
        "    return x_train, y_train, x_test, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz09FqISr0c8"
      },
      "source": [
        "Our data is going to be stored simply in the four variables: `x_train`, `x_test`, `y_train`, and `y_test`.\n",
        "\n",
        "\n",
        "*   Training set: `x_train` is composed of 50,000 images where `y_train` references the corresponding labels.\n",
        "*   Testing set: `x_test` is composed of 10,000 images where `y_test` references the corresponding labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgUtLKmJuTDs",
        "outputId": "9afa5677-775b-4184-ba68-1758f6f3fbda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape:  (49000, 3072)\n",
            "Validation data shape:  (1000, 3072)\n",
            "Test data shape:  (1000, 3072)\n",
            "dev data shape:  (500, 3072)\n",
            "torch.Size([49000, 3073]) torch.Size([1000, 3073]) torch.Size([1000, 3073]) torch.Size([500, 3073])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "num_train = 50000\n",
        "num_test = 5000\n",
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "x_train, y_train, x_test, y_test = cifar10(num_train, num_test)\n",
        "\n",
        "# Split the data into train, val, and test sets. In addition we will\n",
        "# create a small development set as a subset of the training data;\n",
        "# we can use this for development so our code runs faster.\n",
        "num_training = 49000\n",
        "num_validation = 1000\n",
        "num_test = 1000\n",
        "num_dev = 500\n",
        "\n",
        "x_train_np = x_train.numpy()\n",
        "y_train_np = y_train.numpy()\n",
        "x_test_np = x_test.numpy()\n",
        "y_test_np = y_test.numpy()\n",
        "\n",
        "# Our validation set will be num_validation points from the original\n",
        "# training set.\n",
        "mask = range(num_training, num_training + num_validation)\n",
        "X_val = x_train_np[mask]\n",
        "y_val = y_train_np[mask]\n",
        "\n",
        "# Our training set will be the first num_train points from the original\n",
        "# training set.\n",
        "mask = range(num_training)\n",
        "X_train = x_train_np[mask]\n",
        "y_train = y_train_np[mask]\n",
        "\n",
        "# We will also make a development set, which is a small subset of\n",
        "# the training set.\n",
        "mask = np.random.choice(num_training, num_dev, replace=False)\n",
        "X_dev = x_train_np[mask]\n",
        "y_dev = y_train_np[mask]\n",
        "\n",
        "# We use the first num_test points of the original test set as our\n",
        "# test set.\n",
        "mask = range(num_test)\n",
        "X_test = x_test_np[mask]\n",
        "y_test = y_test_np[mask]\n",
        "\n",
        "# Preprocessing: reshape the image data into rows\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
        "X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
        "\n",
        "# As a sanity check, print out the shapes of the data\n",
        "print('Training data shape: ', X_train.shape)\n",
        "print('Validation data shape: ', X_val.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('dev data shape: ', X_dev.shape)\n",
        "\n",
        "# Preprocessing: subtract the mean image\n",
        "# first: compute the image mean based on the training data\n",
        "mean_image = np.mean(X_train, axis=0)\n",
        "\n",
        "# second: subtract the mean image from train and test data\n",
        "X_train -= mean_image\n",
        "X_val -= mean_image\n",
        "X_test -= mean_image\n",
        "X_dev -= mean_image\n",
        "\n",
        "# third: append the bias dimension of ones (i.e. bias trick) so that our SVM\n",
        "# only has to worry about optimizing a single weight matrix W.\n",
        "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
        "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
        "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
        "X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
        "\n",
        "X_train, X_test, X_dev, X_val = torch.FloatTensor(X_train), torch.FloatTensor(X_test), torch.FloatTensor(X_dev), torch.FloatTensor(X_val)\n",
        "y_train, y_test, y_dev, y_val = torch.LongTensor(y_train), torch.LongTensor(y_test), torch.LongTensor(y_dev), torch.LongTensor(y_val)\n",
        "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr_Wv6Hi3Y_K"
      },
      "source": [
        "# k-Nearest Neighbor (kNN) (20 pts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp9dU9BxtSsd"
      },
      "source": [
        "## **Subsampling**\n",
        "\n",
        "When implementing machine learning algorithms, it's usually a good idea to use a small sample of the full dataset. This way your code will run much faster, allowing for more interactive and efficient development. Once you are satisfied that you have correctly implemented the algorithm, you can then rerun with the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HnkODySthr0",
        "outputId": "39d05f07-3e1e-4cc1-e377-6e6d8570e400"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([500, 3073]) torch.Size([500])\n"
          ]
        }
      ],
      "source": [
        "# Subsample size\n",
        "def subsample(X, y, n):\n",
        "    assert len(X) == len(y)\n",
        "    indices = torch.randint(len(X), (n,))\n",
        "    return X[indices],  y[indices]\n",
        "ss_x_train, ss_y_train = subsample(X_train, y_train, 500)\n",
        "print(ss_x_train.shape, ss_y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hPrEvn9FSPf"
      },
      "source": [
        "## Compute Distance (5 pts)\n",
        "\n",
        "Now that we have examined and prepared our data, it is time to implement the Weighted-kNN classifier. We can break the process down into two steps:\n",
        "1. Compute the consine similarities between all training examples and all test examples\n",
        "2. Given these pre-computed similarities, for each test example find its k nearest neighbors and have them vote for the label to output\n",
        "\n",
        "**NOTE**: When implementing algorithms in PyTorch, it's best to avoid loops in Python if possible. Instead it is preferable to implement your computation so that all loops happen inside PyTorch functions. This will usually be much faster than writing your own loops in Python, since PyTorch functions can be internally optimized to iterate efficiently, possibly using multiple threads. This is especially important when using a GPU to accelerate your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "rxg2Aq1pt9fy"
      },
      "outputs": [],
      "source": [
        "def compute_distances(x_train, x_test):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    x_train: shape (num_train, C, H, W) tensor.\n",
        "    x_test: shape (num_test, C, H, W) tensor.\n",
        "\n",
        "    Returns:\n",
        "    dists: shape (num_train, num_test) tensor where dists[j, i] is the\n",
        "        cosine similarity between the ith training image and the jth test\n",
        "        image.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the number of training and testing images\n",
        "    num_train = x_train.shape[0]\n",
        "    num_test = x_test.shape[0]\n",
        "\n",
        "    # dists will be the tensor housing all distance measurements between testing and training\n",
        "    dists = x_train.new_zeros(num_train, num_test)\n",
        "\n",
        "    # Flatten tensors\n",
        "    train = x_train.flatten(1)\n",
        "    test = x_test.flatten(1)\n",
        "\n",
        "    #######################################################################\n",
        "    # TODO (5 pts):\n",
        "    # find the consine similarities between testing and training images,\n",
        "    # and save the computed distance in dists.\n",
        "    #######################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    # Compute dot product between train and test samples\n",
        "    dot_product = train @ test.T  # (num_train, num_test)\n",
        "\n",
        "    # Compute the norms (L2 norms)\n",
        "    train_norms = torch.norm(train, dim=1, keepdim=True)  # (num_train, 1)\n",
        "    test_norms = torch.norm(test, dim=1, keepdim=True)    # (num_test, 1)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    dists = dot_product / (train_norms * test_norms.T)  # (num_train, num_test)\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    return dists"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1i6QyD4j6bB"
      },
      "source": [
        "## Implement Weighted-kNN (10 pts)\n",
        "\n",
        "The Weighted-kNN classifier consists of two stages:\n",
        "\n",
        "*   Training: the classifier takes the training data and simply remembers it\n",
        "*   Testing: For each test sample, the classifier computes the similarity to all training samples and selects the k most similar neighbors. Instead of simple majority voting, each neighbor contributes to the final prediction based on its similarity with the test sample. This ensures that more similar neighbors have a greater influence on the classification decision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "UrYRDVcuuL8m"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "class KnnClassifier:\n",
        "    def __init__(self, x_train, y_train):\n",
        "        \"\"\"\n",
        "        x_train: shape (num_train, C, H, W) tensor where num_train is batch size,\n",
        "          C is channel size, H is height, and W is width.\n",
        "        y_train: shape (num_train) tensor where num_train is batch size providing labels\n",
        "        \"\"\"\n",
        "\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "\n",
        "    def predict(self, x_test, k=1):\n",
        "        \"\"\"\n",
        "        x_test: shape (num_test, C, H, W) tensor where num_test is batch size,\n",
        "          C is channel size, H is height, and W is width.\n",
        "        k: The number of neighbors to use for prediction\n",
        "        \"\"\"\n",
        "\n",
        "        # Init output shape\n",
        "        y_test_pred = torch.zeros(x_test.shape[0], dtype=torch.int64)\n",
        "\n",
        "        # Find & store Euclidean distance between test & train\n",
        "        dists = compute_distances(self.x_train, x_test)\n",
        "\n",
        "        #######################################################################\n",
        "        # TODO (10 pts):\n",
        "        # The goal is to return a tensor y_test_pred where the ith index\n",
        "        # is the assigned label to ith test image by the kNN algorithm.\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        # 1. Index over test images\n",
        "\n",
        "        # 2. Find the indices of the k most similar training samples (highest cosine similarity).\n",
        "\n",
        "        # 3. Retrieve the labels of these k neighbors and compute their contributions as the similarity scores\n",
        "\n",
        "        # 4. Assign the label with the highest accumulated weight as the final prediction.\n",
        "        #######################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        # 1. Index over test images\n",
        "        for i in range(x_test.shape[0]):\n",
        "            # 2. Find the indices of the k most similar training samples (highest cosine similarity).\n",
        "            top_k_indices = torch.argsort(dists[:, i], descending=True)[:k]\n",
        "\n",
        "            # 3. Retrieve the labels of these k neighbors and compute their contributions as the similarity scores\n",
        "            top_k_labels = self.y_train[top_k_indices]\n",
        "            top_k_weights = dists[top_k_indices, i]  # Use similarity as weight\n",
        "\n",
        "            # 4. Assign the label with the highest accumulated weight as the final prediction.\n",
        "            label_weights = defaultdict(float)\n",
        "            for label, weight in zip(top_k_labels, top_k_weights):\n",
        "                label_weights[label.item()] += weight.item()\n",
        "\n",
        "            y_test_pred[i] = max(label_weights, key=label_weights.get)\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "        return y_test_pred\n",
        "\n",
        "    def check_accuracy(self, x_test, y_test, k=1, quiet=False):\n",
        "        \"\"\"\n",
        "        x_test: shape (num_test, C, H, W) tensor where num_test is batch size,\n",
        "          C is channel size, H is height, and W is width.\n",
        "        y_test: shape (num_test) tensor where num_test is batch size providing labels\n",
        "        k: The number of neighbors to use for prediction\n",
        "        quiet: If True, don't print a message.\n",
        "\n",
        "        Returns:\n",
        "        accuracy: Accuracy of this classifier on the test data, as a percent.\n",
        "          Python float in the range [0, 100]\n",
        "        \"\"\"\n",
        "\n",
        "        y_test_pred = self.predict(x_test, k=k)\n",
        "        num_samples = x_test.shape[0]\n",
        "        num_correct = (y_test == y_test_pred).sum().item()\n",
        "        accuracy = 100.0 * num_correct / num_samples\n",
        "        msg = (f'Got {num_correct} / {num_samples} correct; '\n",
        "              f'accuracy is {accuracy:.2f}%')\n",
        "        if not quiet:\n",
        "          print(msg)\n",
        "        return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpaBIPfczlo5"
      },
      "source": [
        "We've finished implementing kNN and can begin testing the algorithm on larger portions of the dataset to see how well it performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXqBT2ZnzDFq",
        "outputId": "9bc8bbe7-6640-4948-b073-5e8bb0252507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 168 / 500 correct; accuracy is 33.60%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33.6"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "num_train = 5000\n",
        "num_test = 500\n",
        "num_val = 500\n",
        "knn_x_train, knn_y_train = subsample(X_train, y_train, num_train)\n",
        "knn_x_test, knn_y_test = subsample(X_test, y_test, num_test)\n",
        "knn_x_val, knn_y_val = subsample(X_val, y_val, num_val)\n",
        "classifier = KnnClassifier(knn_x_train, knn_y_train)\n",
        "classifier.check_accuracy(knn_x_test, knn_y_test, k=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CQ9PNc0uTDu"
      },
      "source": [
        "## Hyperparameter Tuning (5 pts)\n",
        "\n",
        "Now we use the validation set to tune hyperparameters (number of nearest neighbors k). You should experiment with different ranges of k."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIE7dtR_uTDv",
        "outputId": "88be9e64-7ef8-48c6-9ada-fcdff2fc1b42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k 1 val accuracy: 28.600000\n",
            "k 3 val accuracy: 30.600000\n",
            "k 5 val accuracy: 34.200000\n",
            "k 7 val accuracy: 34.200000\n",
            "k 9 val accuracy: 33.800000\n",
            "k 15 val accuracy: 33.400000\n",
            "k 20 val accuracy: 33.600000\n",
            "k 50 val accuracy: 34.200000\n",
            "best validation accuracy achieved: 34.200000\n",
            "Got 168 / 500 correct; accuracy is 33.60%\n",
            "final test accuracy knn achieved: 33.600000\n"
          ]
        }
      ],
      "source": [
        "results = {}\n",
        "best_val = -1   # The highest validation accuracy that we have seen so far.\n",
        "best_k = None # The value of k that achieved the highest validation rate.\n",
        "\n",
        "################################################################################\n",
        "# TODO (5 pts):                                                               #\n",
        "# Write code that chooses the best k value by tuning on the validation         #\n",
        "# set. For each value of k, train a KnnClassifier on the                       #\n",
        "# training set, compute its accuracy on the training and validation sets, and  #\n",
        "# store these numbers in the results dictionary. In addition, store the best   #\n",
        "# validation accuracy in best_val and the best value of k in best_k.           #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "# fill in your own values\n",
        "k_choices = [1, 3, 5, 7, 9, 15, 20, 50]\n",
        "for k in k_choices:\n",
        "    # Train classifier with current k\n",
        "    classifier = KnnClassifier(knn_x_train, knn_y_train)\n",
        "\n",
        "    # Compute accuracy on validation set\n",
        "    val_accuracy = classifier.check_accuracy(knn_x_val, knn_y_val, k=k, quiet=True)\n",
        "\n",
        "    # Store results\n",
        "    results[k] = val_accuracy\n",
        "\n",
        "    # Track best validation accuracy\n",
        "    if val_accuracy > best_val:\n",
        "        best_val = val_accuracy\n",
        "        best_k = k\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "# Print out results.\n",
        "for k in sorted(results):\n",
        "    val_accuracy = results[k]\n",
        "    print('k %d val accuracy: %f' % (\n",
        "                k, val_accuracy))\n",
        "\n",
        "print('best validation accuracy achieved: %f' % best_val)\n",
        "\n",
        "classifier = KnnClassifier(knn_x_train, knn_y_train)\n",
        "test_acc = classifier.check_accuracy(knn_x_test, knn_y_test, k=best_k)\n",
        "print('final test accuracy knn achieved: %f' % test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8Pr5Oh5rlUq"
      },
      "source": [
        "# Define a General Classifier Class (15 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fZSgxj-s_jf"
      },
      "source": [
        "Before implementing Support Vector Machine (SVM) Classifier. We define a general classifier class that contains the following main functions:\n",
        "\n",
        "\n",
        "1.   `train`: train this linear classifier using stochastic gradient descent.\n",
        "2.   `predict`: use the trained weights of this linear classifier to predict labels for data points.\n",
        "3.   `loss`: compute the loss function and its derivative.\n",
        "\n",
        "We will define SVM and Softmax classifier as subclasses of this general linear classifier class. Subclasses will override the `loss` function.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "O0Id2-q_QF02"
      },
      "outputs": [],
      "source": [
        "class LinearClassifier(object):\n",
        "    def __init__(self):\n",
        "        self.W = None\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        X,\n",
        "        y,\n",
        "        learning_rate=1e-3,\n",
        "        reg=1e-5,\n",
        "        num_iters=100,\n",
        "        batch_size=200,\n",
        "        verbose=False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Train this linear classifier using stochastic gradient descent.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "          training samples each of dimension D.\n",
        "        - y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
        "          means that X[i] has label 0 <= c < C for C classes.\n",
        "        - learning_rate: (float) learning rate for optimization.\n",
        "        - reg: (float) regularization strength.\n",
        "        - num_iters: (integer) number of steps to take when optimizing\n",
        "        - batch_size: (integer) number of training examples to use at each step.\n",
        "        - verbose: (boolean) If true, print progress during optimization.\n",
        "\n",
        "        Outputs:\n",
        "        A list containing the value of the loss function at each training iteration.\n",
        "        \"\"\"\n",
        "        num_train, dim = X.shape\n",
        "        num_classes = (\n",
        "            np.max(y) + 1\n",
        "        )  # assume y takes values 0...K-1 where K is number of classes\n",
        "        if self.W is None:\n",
        "            # lazily initialize W\n",
        "            self.W = 0.001 * np.random.randn(dim, num_classes)\n",
        "\n",
        "        # Run stochastic gradient descent to optimize W\n",
        "        loss_history = []\n",
        "        for it in range(num_iters):\n",
        "            X_batch = None\n",
        "            y_batch = None\n",
        "\n",
        "            #########################################################################\n",
        "            # TODO (5 pts):                                                        #\n",
        "            # Sample batch_size elements from the training data and their           #\n",
        "            # corresponding labels to use in this round of gradient descent.        #\n",
        "            # Store the data in X_batch and their corresponding labels in           #\n",
        "            # y_batch; after sampling X_batch should have shape (batch_size, dim)   #\n",
        "            # and y_batch should have shape (batch_size,)                           #\n",
        "            #########################################################################\n",
        "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "            batch_indices = np.random.choice(num_train, batch_size, replace=True)\n",
        "            X_batch = X[batch_indices]\n",
        "            y_batch = y[batch_indices]\n",
        "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "            # evaluate loss and gradient\n",
        "            loss, grad = self.loss(X_batch, y_batch, reg)\n",
        "            loss_history.append(loss)\n",
        "\n",
        "            # perform parameter update\n",
        "            #########################################################################\n",
        "            # TODO (5 pts):                                                         #\n",
        "            # Update the weights using the gradient and the learning rate.          #\n",
        "            #########################################################################\n",
        "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "            self.W -= learning_rate * grad\n",
        "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "            if verbose and it % 100 == 0:\n",
        "                print(\"iteration %d / %d: loss %f\" % (it, num_iters, loss))\n",
        "\n",
        "        return loss_history\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Use the trained weights of this linear classifier to predict labels for\n",
        "        data points.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "          training samples each of dimension D.\n",
        "\n",
        "        Returns:\n",
        "        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
        "          array of length N, and each element is an integer giving the predicted\n",
        "          class.\n",
        "        \"\"\"\n",
        "        y_pred = np.zeros(X.shape[0])\n",
        "        ###########################################################################\n",
        "        # TODO (5 pts):                                                           #\n",
        "        # Implement this method. Store the predicted labels in y_pred.            #\n",
        "        ###########################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        scores = X @ self.W  # Compute class scores\n",
        "        y_pred = np.argmax(scores, axis=1)  # Select class with highest score\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        return y_pred\n",
        "\n",
        "    def loss(self, X_batch, y_batch, reg):\n",
        "        \"\"\"\n",
        "        Compute the loss function and its derivative.\n",
        "        Subclasses will override this.\n",
        "\n",
        "        Inputs:\n",
        "        - X_batch: A numpy array of shape (N, D) containing a minibatch of N\n",
        "          data points; each point has dimension D.\n",
        "        - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
        "        - reg: (float) regularization strength.\n",
        "\n",
        "        Returns: A tuple containing:\n",
        "        - loss as a single float\n",
        "        - gradient with respect to self.W; an array of the same shape as W\n",
        "        \"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcO2mGt3NQ9n"
      },
      "source": [
        "# Multiclass Support Vector Machine (SVM) (25 pts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3_Z4W_ZH59m"
      },
      "source": [
        "[Support vector machines (SVMs)](https://scikit-learn.org/stable/modules/svm.html) are a set of supervised learning methods used for classification.\n",
        "\n",
        "The advantages of support vector machines are:\n",
        "\n",
        "* Effective in high dimensional spaces.\n",
        "* Still effective in cases where number of dimensions is greater than the number of samples.\n",
        "* Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
        "* Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
        "\n",
        "The disadvantages of support vector machines include:\n",
        "\n",
        "* If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
        "* SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below).\n",
        "\n",
        "In this section, we will first implement the loss function for SVM and use the validation set to tune hyperparameters.\n",
        "\n",
        "**NOTE:** please use [numpy](https://numpy.org/), please do not use [scikit-learn](https://scikit-learn.org/stable/), [PyTorch](https://pytorch.org/) or other libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2ajMlXEIjWz"
      },
      "source": [
        "## Loss Function (20 pts)\n",
        "\n",
        "We first structure the loss function for SVM. For detailed explanations of SVM loss, please check out [this reading material](https://cs231n.github.io/linear-classify/#loss-function)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "edfvcABEN_np"
      },
      "outputs": [],
      "source": [
        "def svm_loss(W, X, y, reg):\n",
        "    \"\"\"\n",
        "    Structured SVM loss function implementation.\n",
        "\n",
        "    Inputs have dimension D, there are C classes, and we operate on minibatches\n",
        "    of N examples.\n",
        "\n",
        "    Inputs:\n",
        "    - W: A numpy array of shape (D, C) containing weights.\n",
        "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
        "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
        "      that X[i] has label c, where 0 <= c < C.\n",
        "    - reg: (float) regularization strength\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - loss as single float\n",
        "    - gradient with respect to weights W; an array of same shape as W\n",
        "    \"\"\"\n",
        "    loss = 0.0\n",
        "    dW = np.zeros(W.shape)  # initialize the gradient as zero\n",
        "\n",
        "    #############################################################################\n",
        "    # TODO (10 pts):                                                            #\n",
        "    # Implement a vectorized version of the structured SVM loss, storing the    #\n",
        "    # result in loss. Refer to https://cs231n.github.io/linear-classify/        #\n",
        "    #############################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    # Get number of training samples\n",
        "    num_train = X.shape[0]  # Fix potential issue with previous num_train\n",
        "\n",
        "    # Compute class scores\n",
        "    scores = X @ W  # Shape (N, C)\n",
        "\n",
        "    # Ensure y is an integer for indexing\n",
        "    y = y.astype(int)\n",
        "\n",
        "    # Extract correct class scores\n",
        "    correct_class_scores = scores[np.arange(num_train), y].reshape(-1, 1)  # Shape (N, 1)\n",
        "\n",
        "    # Compute margin (max(0, s_j - s_{y_i} + 1))\n",
        "    margins = np.maximum(0, scores - correct_class_scores + 1)  # Shape (N, C)\n",
        "    margins[np.arange(num_train), y] = 0  # Ignore correct class\n",
        "\n",
        "    # Compute loss\n",
        "    loss = np.sum(margins) / num_train  # Average over all samples\n",
        "    loss += reg * np.sum(W ** 2)  # Add regularization term\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    #############################################################################\n",
        "    # TODO (10 pts):                                                            #\n",
        "    # Implement a vectorized version of the gradient for the structured SVM     #\n",
        "    # loss, storing the result in dW.                                           #\n",
        "    #                                                                           #\n",
        "    # Hint: Instead of computing the gradient from scratch, it may be easier    #\n",
        "    # to reuse some of the intermediate values that you used to compute the     #\n",
        "    # loss.                                                                     #\n",
        "    #############################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    # Compute mask where margins > 0\n",
        "    margin_mask = (margins > 0).astype(float)\n",
        "\n",
        "    # Count incorrect classifications for each sample\n",
        "    margin_mask[np.arange(num_train), y] = -np.sum(margin_mask, axis=1)\n",
        "\n",
        "    # Compute gradient\n",
        "    dW = (X.T @ margin_mask) / num_train  # Average over all samples\n",
        "    dW += 2 * reg * W  # Regularization term\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    return loss, dW\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlw6rH_mQwAa"
      },
      "source": [
        "Now, we can test our implementation of SVM loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zu2PYRB2QbPC",
        "outputId": "bc73d5cd-3112-4b2d-fbe1-9fd5a122998b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 8.998826e+00 computed in 0.020505s\n"
          ]
        }
      ],
      "source": [
        "# generate a random SVM weight matrix of small numbers\n",
        "W = np.random.randn(3073, 10) * 0.0001\n",
        "\n",
        "tic = time.time()\n",
        "loss, _ = svm_loss(W, X_dev.numpy(), y_dev.numpy(), 0.000005)\n",
        "toc = time.time()\n",
        "print('loss: %e computed in %fs' % (loss, toc - tic))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "zWpju06hQIYX"
      },
      "outputs": [],
      "source": [
        "class LinearSVM(LinearClassifier):\n",
        "    \"\"\" A subclass that uses the Multiclass SVM loss function \"\"\"\n",
        "\n",
        "    def loss(self, X_batch, y_batch, reg):\n",
        "        return svm_loss(self.W, X_batch, y_batch, reg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFopfak1QPI6",
        "outputId": "63d98dd0-40ff-421b-e069-7cfcdb6a5bf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0 / 1500: loss 782.116353\n",
            "iteration 100 / 1500: loss 292.702711\n",
            "iteration 200 / 1500: loss 113.105350\n",
            "iteration 300 / 1500: loss 47.201874\n",
            "iteration 400 / 1500: loss 23.017616\n",
            "iteration 500 / 1500: loss 14.143208\n",
            "iteration 600 / 1500: loss 10.885566\n",
            "iteration 700 / 1500: loss 9.691425\n",
            "iteration 800 / 1500: loss 9.252887\n",
            "iteration 900 / 1500: loss 9.092204\n",
            "iteration 1000 / 1500: loss 9.032534\n",
            "iteration 1100 / 1500: loss 9.011159\n",
            "iteration 1200 / 1500: loss 9.003135\n",
            "iteration 1300 / 1500: loss 9.000165\n",
            "iteration 1400 / 1500: loss 8.999247\n",
            "That took 8.266896s\n"
          ]
        }
      ],
      "source": [
        "svm = LinearSVM()\n",
        "tic = time.time()\n",
        "loss_hist = svm.train(X_train.numpy(), y_train.numpy(), learning_rate=1e-7, reg=2.5e4,\n",
        "                      num_iters=1500, verbose=True)\n",
        "toc = time.time()\n",
        "print('That took %fs' % (toc - tic))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwJY1JLmQeBQ",
        "outputId": "76b59e6d-5e16-469b-ec50-13cb5bb9dec5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training accuracy: 0.242735\n",
            "validation accuracy: 0.263000\n"
          ]
        }
      ],
      "source": [
        "y_train_pred = svm.predict(X_train.numpy())\n",
        "print('training accuracy: %f' % (np.mean(y_train.numpy() == y_train_pred), ))\n",
        "y_val_pred = svm.predict(X_val.numpy())\n",
        "print('validation accuracy: %f' % (np.mean(y_val.numpy() == y_val_pred), ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlA3kc3BIr2V"
      },
      "source": [
        "## Hyperparameter Tuning (5 pts)\n",
        "\n",
        "Now we use the validation set to tune hyperparameters (regularization strength and learning rate). You should experiment with different ranges for the learning rates and regularization strengths.\n",
        "\n",
        "**Note:** you may see runtime/overflow warnings during hyper-parameter search. This may be caused by extreme values, and is not a bug."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# results is dictionary mapping tuples of the form\n",
        "# (learning_rate, regularization_strength) to tuples of the form\n",
        "# (training_accuracy, validation_accuracy). The accuracy is simply the fraction\n",
        "# of data points that are correctly classified.\n",
        "results = {}\n",
        "best_val = -1   # The highest validation accuracy that we have seen so far.\n",
        "best_svm = None # The LinearSVM object that achieved the highest validation rate.\n",
        "\n",
        "################################################################################\n",
        "# TODO (10 pts):                                                               #\n",
        "# Write code that chooses the best hyperparameters by tuning on the validation #\n",
        "# set. For each combination of hyperparameters, train a linear SVM on the      #\n",
        "# training set, compute its accuracy on the training and validation sets, and  #\n",
        "# store these numbers in the results dictionary. In addition, store the best   #\n",
        "# validation accuracy in best_val and the LinearSVM object that achieves this  #\n",
        "# accuracy in best_svm.                                                        #\n",
        "#                                                                              #\n",
        "# Hint: You should use a small value for num_iters as you develop your         #\n",
        "# validation code so that the SVMs don't take much time to train; once you are #\n",
        "# confident that your validation code works, you should rerun the validation   #\n",
        "# code with a larger value for num_iters.\n",
        "                                      #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "learning_rates = [0.005, 0.008, 0.01, 0.012, 0.015]\n",
        "regularization_strengths = [5e-4, 8e-4, 1e-3, 3e-3, 5e-3]\n",
        "\n",
        "# Try all combinations of hyperparameters\n",
        "for lr in learning_rates:\n",
        "   for reg in regularization_strengths:\n",
        "       svm = LinearSVM()\n",
        "\n",
        "       # Train with more iterations for better convergence\n",
        "       loss_hist = svm.train(X_train.numpy(), y_train.numpy(),\n",
        "                         learning_rate=lr, reg=reg,\n",
        "                         num_iters=1500,\n",
        "                         batch_size=200,\n",
        "                         verbose=False)\n",
        "\n",
        "       # Compute accuracies\n",
        "       y_train_pred = svm.predict(X_train.numpy())\n",
        "       train_acc = np.mean(y_train.numpy() == y_train_pred)\n",
        "\n",
        "       y_val_pred = svm.predict(X_val.numpy())\n",
        "       val_acc = np.mean(y_val.numpy() == y_val_pred)\n",
        "\n",
        "       # Store results\n",
        "       results[(lr, reg)] = (train_acc, val_acc)\n",
        "\n",
        "       # Print current result to see progress\n",
        "       print(f\"lr {lr:.1e}, reg {reg:.1e}: train acc: {train_acc:.4f}, val acc: {val_acc:.4f}\")\n",
        "\n",
        "       # Update best model if validation accuracy improves\n",
        "       if val_acc > best_val:\n",
        "           best_val = val_acc\n",
        "           best_svm = svm\n",
        "           print(f\"New best: {val_acc:.4f}\")\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "# Print out results.\n",
        "for lr, reg in sorted(results):\n",
        "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
        "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
        "                lr, reg, train_accuracy, val_accuracy))\n",
        "\n",
        "print('best validation accuracy achieved: %f' % best_val)\n",
        "y_test_pred = best_svm.predict(X_test.numpy())\n",
        "test_acc = np.mean(y_test.numpy() == y_test_pred)\n",
        "print('final test accuracy svm achieved: %f' % test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynGGV0gEmouf",
        "outputId": "b5f42319-12c8-4d25-9112-2696a2f81ac7"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr 5.0e-03, reg 5.0e-04: train acc: 0.4002, val acc: 0.4030\n",
            "New best: 0.4030\n",
            "lr 5.0e-03, reg 8.0e-04: train acc: 0.3996, val acc: 0.3880\n",
            "lr 5.0e-03, reg 1.0e-03: train acc: 0.4023, val acc: 0.3900\n",
            "lr 5.0e-03, reg 3.0e-03: train acc: 0.4010, val acc: 0.3980\n",
            "lr 5.0e-03, reg 5.0e-03: train acc: 0.4014, val acc: 0.4040\n",
            "New best: 0.4040\n",
            "lr 8.0e-03, reg 5.0e-04: train acc: 0.3989, val acc: 0.3860\n",
            "lr 8.0e-03, reg 8.0e-04: train acc: 0.4080, val acc: 0.3930\n",
            "lr 8.0e-03, reg 1.0e-03: train acc: 0.4037, val acc: 0.4030\n",
            "lr 8.0e-03, reg 3.0e-03: train acc: 0.4058, val acc: 0.4030\n",
            "lr 8.0e-03, reg 5.0e-03: train acc: 0.4051, val acc: 0.4110\n",
            "New best: 0.4110\n",
            "lr 1.0e-02, reg 5.0e-04: train acc: 0.4079, val acc: 0.3990\n",
            "lr 1.0e-02, reg 8.0e-04: train acc: 0.4069, val acc: 0.4070\n",
            "lr 1.0e-02, reg 1.0e-03: train acc: 0.4042, val acc: 0.4010\n",
            "lr 1.0e-02, reg 3.0e-03: train acc: 0.4090, val acc: 0.3950\n",
            "lr 1.0e-02, reg 5.0e-03: train acc: 0.4017, val acc: 0.4020\n",
            "lr 1.2e-02, reg 5.0e-04: train acc: 0.4106, val acc: 0.4120\n",
            "New best: 0.4120\n",
            "lr 1.2e-02, reg 8.0e-04: train acc: 0.4082, val acc: 0.3930\n",
            "lr 1.2e-02, reg 1.0e-03: train acc: 0.4104, val acc: 0.4010\n",
            "lr 1.2e-02, reg 3.0e-03: train acc: 0.4058, val acc: 0.3940\n",
            "lr 1.2e-02, reg 5.0e-03: train acc: 0.4081, val acc: 0.3950\n",
            "lr 1.5e-02, reg 5.0e-04: train acc: 0.3963, val acc: 0.3900\n",
            "lr 1.5e-02, reg 8.0e-04: train acc: 0.4090, val acc: 0.4070\n",
            "lr 1.5e-02, reg 1.0e-03: train acc: 0.4089, val acc: 0.4030\n",
            "lr 1.5e-02, reg 3.0e-03: train acc: 0.3987, val acc: 0.3850\n",
            "lr 1.5e-02, reg 5.0e-03: train acc: 0.4079, val acc: 0.3970\n",
            "lr 5.000000e-03 reg 5.000000e-04 train accuracy: 0.400224 val accuracy: 0.403000\n",
            "lr 5.000000e-03 reg 8.000000e-04 train accuracy: 0.399571 val accuracy: 0.388000\n",
            "lr 5.000000e-03 reg 1.000000e-03 train accuracy: 0.402265 val accuracy: 0.390000\n",
            "lr 5.000000e-03 reg 3.000000e-03 train accuracy: 0.401041 val accuracy: 0.398000\n",
            "lr 5.000000e-03 reg 5.000000e-03 train accuracy: 0.401367 val accuracy: 0.404000\n",
            "lr 8.000000e-03 reg 5.000000e-04 train accuracy: 0.398939 val accuracy: 0.386000\n",
            "lr 8.000000e-03 reg 8.000000e-04 train accuracy: 0.407980 val accuracy: 0.393000\n",
            "lr 8.000000e-03 reg 1.000000e-03 train accuracy: 0.403714 val accuracy: 0.403000\n",
            "lr 8.000000e-03 reg 3.000000e-03 train accuracy: 0.405776 val accuracy: 0.403000\n",
            "lr 8.000000e-03 reg 5.000000e-03 train accuracy: 0.405082 val accuracy: 0.411000\n",
            "lr 1.000000e-02 reg 5.000000e-04 train accuracy: 0.407898 val accuracy: 0.399000\n",
            "lr 1.000000e-02 reg 8.000000e-04 train accuracy: 0.406878 val accuracy: 0.407000\n",
            "lr 1.000000e-02 reg 1.000000e-03 train accuracy: 0.404224 val accuracy: 0.401000\n",
            "lr 1.000000e-02 reg 3.000000e-03 train accuracy: 0.409000 val accuracy: 0.395000\n",
            "lr 1.000000e-02 reg 5.000000e-03 train accuracy: 0.401735 val accuracy: 0.402000\n",
            "lr 1.200000e-02 reg 5.000000e-04 train accuracy: 0.410633 val accuracy: 0.412000\n",
            "lr 1.200000e-02 reg 8.000000e-04 train accuracy: 0.408245 val accuracy: 0.393000\n",
            "lr 1.200000e-02 reg 1.000000e-03 train accuracy: 0.410388 val accuracy: 0.401000\n",
            "lr 1.200000e-02 reg 3.000000e-03 train accuracy: 0.405755 val accuracy: 0.394000\n",
            "lr 1.200000e-02 reg 5.000000e-03 train accuracy: 0.408102 val accuracy: 0.395000\n",
            "lr 1.500000e-02 reg 5.000000e-04 train accuracy: 0.396265 val accuracy: 0.390000\n",
            "lr 1.500000e-02 reg 8.000000e-04 train accuracy: 0.408980 val accuracy: 0.407000\n",
            "lr 1.500000e-02 reg 1.000000e-03 train accuracy: 0.408898 val accuracy: 0.403000\n",
            "lr 1.500000e-02 reg 3.000000e-03 train accuracy: 0.398714 val accuracy: 0.385000\n",
            "lr 1.500000e-02 reg 5.000000e-03 train accuracy: 0.407878 val accuracy: 0.397000\n",
            "best validation accuracy achieved: 0.412000\n",
            "final test accuracy svm achieved: 0.384000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZwG46WSuTDx"
      },
      "source": [
        "# Implementing a Neural Network (40 pts)\n",
        "In this exercise we will develop a neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset.\n",
        "\n",
        "We train the network with a cross-entropy loss function and L2 regularization on the weight matrices. The network uses a Sigmoid nonlinearity after the first fully connected layer.\n",
        "\n",
        "In other words, the network has the following architecture:\n",
        "\n",
        "  input -> fully connected layer -> Sigmoid -> fully connected layer -> softmax -> cross-entropy\n",
        "\n",
        "The outputs of the second fully-connected layer are the scores for each class.\n",
        "\n",
        "**Note**: When you implement the regularization over W, **please DO NOT multiply the regularization term by 1/2** (no coefficient).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "WqqXXcGJuTDx"
      },
      "outputs": [],
      "source": [
        "# Template class modules that we will use later: Do not edit/modify this class\n",
        "class TwoLayerNet(object):\n",
        "  def __init__(self, input_size, hidden_size, output_size,\n",
        "               dtype=torch.float32, device='cuda', std=1e-4):\n",
        "    \"\"\"\n",
        "    Initialize the model. Weights are initialized to small random values and\n",
        "    biases are initialized to zero. Weights and biases are stored in the\n",
        "    variable self.params, which is a dictionary with the following keys:\n",
        "\n",
        "    W1: First layer weights; has shape (D, H)\n",
        "    b1: First layer biases; has shape (H,)\n",
        "    W2: Second layer weights; has shape (H, C)\n",
        "    b2: Second layer biases; has shape (C,)\n",
        "\n",
        "    Inputs:\n",
        "    - input_size: The dimension D of the input data.\n",
        "    - hidden_size: The number of neurons H in the hidden layer.\n",
        "    - output_size: The number of classes C.\n",
        "    - dtype: Optional, data type of each initial weight params\n",
        "    - device: Optional, whether the weight params is on GPU or CPU\n",
        "    - std: Optional, initial weight scaler.\n",
        "    \"\"\"\n",
        "    # reset seed before start\n",
        "    random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    self.params = {}\n",
        "    self.params['W1'] = std * torch.randn(input_size, hidden_size, dtype=dtype, device=device)\n",
        "    self.params['b1'] = torch.zeros(hidden_size, dtype=dtype, device=device)\n",
        "    self.params['W2'] = std * torch.randn(hidden_size, output_size, dtype=dtype, device=device)\n",
        "    self.params['b2'] = torch.zeros(output_size, dtype=dtype, device=device)\n",
        "\n",
        "  def loss(self, X, y=None, reg=0.0):\n",
        "    return nn_forward_backward(self.params, X, y, reg)\n",
        "\n",
        "  def train(self, X, y, X_val, y_val,\n",
        "            learning_rate=1e-3, learning_rate_decay=0.95,\n",
        "            reg=5e-6, num_iters=100,\n",
        "            batch_size=200, verbose=False):\n",
        "    return nn_train(\n",
        "            self.params,\n",
        "            nn_forward_backward,\n",
        "            nn_predict,\n",
        "            X, y, X_val, y_val,\n",
        "            learning_rate, learning_rate_decay,\n",
        "            reg, num_iters, batch_size, verbose)\n",
        "\n",
        "  def predict(self, X):\n",
        "    return nn_predict(self.params, nn_forward_backward, X)\n",
        "\n",
        "  def save(self, path):\n",
        "    torch.save(self.params, path)\n",
        "    print(\"Saved in {}\".format(path))\n",
        "\n",
        "  def load(self, path):\n",
        "    checkpoint = torch.load(path, map_location='cpu')\n",
        "    self.params = checkpoint\n",
        "    print(\"load checkpoint file: {}\".format(path))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaCR7OYSuTDx"
      },
      "source": [
        "Forward pass function (5 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "8qJ0OTYCuTDy"
      },
      "outputs": [],
      "source": [
        "def nn_forward_pass(params, X):\n",
        "    \"\"\"\n",
        "    The first stage of our neural network implementation: Run the forward pass\n",
        "    of the network to compute the hidden layer features and classification\n",
        "    scores. The network architecture should be:\n",
        "\n",
        "    FC layer -> ReLU (hidden) -> FC layer (scores)\n",
        "\n",
        "    As a practice, we will NOT allow to use torch.relu and torch.nn ops\n",
        "    just for this time (you can use it from A3).\n",
        "\n",
        "    Inputs:\n",
        "    - params: a dictionary of PyTorch Tensor that store the weights of a model.\n",
        "      It should have following keys with shape\n",
        "          W1: First layer weights; has shape (D, H)\n",
        "          b1: First layer biases; has shape (H,)\n",
        "          W2: Second layer weights; has shape (H, C)\n",
        "          b2: Second layer biases; has shape (C,)\n",
        "    - X: Input data of shape (N, D). Each X[i] is a training sample.\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - scores: Tensor of shape (N, C) giving the classification scores for X\n",
        "    - hidden: Tensor of shape (N, H) giving the hidden layer representation\n",
        "      for each input value (after the ReLU).\n",
        "    \"\"\"\n",
        "    # Unpack variables from the params dictionary\n",
        "    W1, b1 = params['W1'], params['b1']\n",
        "    W2, b2 = params['W2'], params['b2']\n",
        "    N, D = X.shape\n",
        "\n",
        "    # Compute the forward pass\n",
        "    hidden = None\n",
        "    scores = None\n",
        "    def activation(z):\n",
        "      # TODO: use sigmoid function [https://en.wikipedia.org/wiki/Sigmoid_function]\n",
        "      return 1.0 / (1.0 + torch.exp(-z))\n",
        "    ############################################################################\n",
        "    # TODO: Perform the forward pass, computing the class scores for the input.#\n",
        "    # Store the result in the scores variable, which should be an tensor of    #\n",
        "    # shape (N, C).                                                            #\n",
        "    ############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    # First layer: linear transformation\n",
        "    z1 = X.mm(W1) + b1\n",
        "\n",
        "    # Apply sigmoid activation\n",
        "    hidden = activation(z1)\n",
        "\n",
        "    # Second layer: linear transformation to get scores\n",
        "    scores = hidden.mm(W2) + b2\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "\n",
        "    return scores, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO14lH1_uTDy"
      },
      "source": [
        "Loss function + Gradients computation (15 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "LiYyBBcKuTDy"
      },
      "outputs": [],
      "source": [
        "def nn_forward_backward(params, X, y=None, reg=0.0):\n",
        "    \"\"\"\n",
        "    Compute the loss and gradients for a two layer fully connected neural\n",
        "    network. When you implement loss and gradient, please don't forget to\n",
        "    scale the losses/gradients by the batch size.\n",
        "\n",
        "    Inputs: First two parameters (params, X) are same as nn_forward_pass\n",
        "    - params: a dictionary of PyTorch Tensor that store the weights of a model.\n",
        "      It should have following keys with shape\n",
        "          W1: First layer weights; has shape (D, H)\n",
        "          b1: First layer biases; has shape (H,)\n",
        "          W2: Second layer weights; has shape (H, C)\n",
        "          b2: Second layer biases; has shape (C,)\n",
        "    - X: Input data of shape (N, D). Each X[i] is a training sample.\n",
        "    - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is\n",
        "      an integer in the range 0 <= y[i] < C. This parameter is optional; if it\n",
        "      is not passed then we only return scores, and if it is passed then we\n",
        "      instead return the loss and gradients.\n",
        "    - reg: Regularization strength.\n",
        "\n",
        "    Returns:\n",
        "    If y is None, return a tensor scores of shape (N, C) where scores[i, c] is\n",
        "    the score for class c on input X[i].\n",
        "\n",
        "    If y is not None, instead return a tuple of:\n",
        "    - loss: Loss (data loss and regularization loss) for this batch of training\n",
        "      samples.\n",
        "    - grads: Dictionary mapping parameter names to gradients of those parameters\n",
        "      with respect to the loss function; has the same keys as self.params.\n",
        "    \"\"\"\n",
        "    # Unpack variables from the params dictionary\n",
        "    W1, b1 = params['W1'], params['b1']\n",
        "    W2, b2 = params['W2'], params['b2']\n",
        "    N, D = X.shape\n",
        "\n",
        "    scores, hidden = nn_forward_pass(params, X)\n",
        "    # If the targets are not given then jump out, we're done\n",
        "    if y is None:\n",
        "      return scores\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = None\n",
        "    ############################################################################\n",
        "    # TODO: Compute the loss, based on the results from nn_forward_pass.       #\n",
        "    # This should include both the data loss and L2 regularization for W1 and  #\n",
        "    # W2. Store the result in the variable loss, which should be a scalar. Use #\n",
        "    # the Cross-entropy classifier loss.                                       #\n",
        "    # Please DO NOT multiply the regularization term by 1/2 (no coefficient).  #\n",
        "    # If you are not careful here, it is easy to run into numeric instability  #\n",
        "    # (Check Numeric Stability in http://cs231n.github.io/linear-classify/).   #\n",
        "    ############################################################################\n",
        "    scores_shifted = scores - scores.max(dim=1, keepdim=True)[0]  # For numerical stability\n",
        "    exp_scores = torch.exp(scores_shifted)\n",
        "    probs = exp_scores / exp_scores.sum(dim=1, keepdim=True)\n",
        "\n",
        "    # Cross-entropy loss\n",
        "    correct_logprobs = -torch.log(probs[torch.arange(N), y])\n",
        "    data_loss = correct_logprobs.sum() / N\n",
        "\n",
        "    # L2 regularization\n",
        "    reg_loss = reg * (torch.sum(W1 * W1) + torch.sum(W2 * W2))\n",
        "\n",
        "    # Total loss\n",
        "    loss = data_loss + reg_loss\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    # Backward pass: compute gradients\n",
        "    grads = {}\n",
        "    ###########################################################################\n",
        "    # TODO: Compute the backward pass, computing the derivatives of the       #\n",
        "    # weights and biases. Store the results in the grads dictionary.          #\n",
        "    # For example, grads['W1'] should store the gradient on W1, and be a      #\n",
        "    # tensor of same size                                                     #\n",
        "    ###########################################################################\n",
        "    # Gradient of the loss with respect to scores\n",
        "    dscores = probs.clone()\n",
        "    dscores[torch.arange(N), y] -= 1\n",
        "    dscores /= N\n",
        "\n",
        "    # Backprop through the second layer\n",
        "    # Gradient with respect to W2 and b2\n",
        "    grads['W2'] = hidden.t().mm(dscores)\n",
        "    grads['b2'] = dscores.sum(dim=0)\n",
        "\n",
        "    # Gradient of the loss with respect to hidden\n",
        "    dhidden = dscores.mm(W2.t())\n",
        "\n",
        "    # Backprop through the sigmoid activation\n",
        "    # Gradient of sigmoid: dsigmoid = sigmoid * (1-sigmoid)\n",
        "    dsigmoid = dhidden * hidden * (1 - hidden)\n",
        "\n",
        "    # Backprop through the first layer\n",
        "    # Gradient with respect to W1 and b1\n",
        "    grads['W1'] = X.t().mm(dsigmoid)\n",
        "    grads['b1'] = dsigmoid.sum(dim=0)\n",
        "\n",
        "    # Add regularization gradient contribution\n",
        "    grads['W2'] += 2 * reg * W2\n",
        "    grads['W1'] += 2 * reg * W1\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "\n",
        "    return loss, grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSxQkC-DuTDy"
      },
      "source": [
        "Weight updates (5 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "oA8tna-FuTDy"
      },
      "outputs": [],
      "source": [
        "def nn_train(params, loss_func, pred_func, X, y, X_val, y_val,\n",
        "            learning_rate=1e-3, learning_rate_decay=0.95,\n",
        "            reg=5e-6, num_iters=100,\n",
        "            batch_size=200, verbose=False):\n",
        "  \"\"\"\n",
        "  Train this neural network using stochastic gradient descent.\n",
        "\n",
        "  Inputs:\n",
        "  - params: a dictionary of PyTorch Tensor that store the weights of a model.\n",
        "    It should have following keys with shape\n",
        "        W1: First layer weights; has shape (D, H)\n",
        "        b1: First layer biases; has shape (H,)\n",
        "        W2: Second layer weights; has shape (H, C)\n",
        "        b2: Second layer biases; has shape (C,)\n",
        "  - loss_func: a loss function that computes the loss and the gradients.\n",
        "    It takes as input:\n",
        "    - params: Same as input to nn_train\n",
        "    - X_batch: A minibatch of inputs of shape (B, D)\n",
        "    - y_batch: Ground-truth labels for X_batch\n",
        "    - reg: Same as input to nn_train\n",
        "    And it returns a tuple of:\n",
        "      - loss: Scalar giving the loss on the minibatch\n",
        "      - grads: Dictionary mapping parameter names to gradients of the loss with\n",
        "        respect to the corresponding parameter.\n",
        "  - pred_func: prediction function that im\n",
        "  - X: A PyTorch tensor of shape (N, D) giving training data.\n",
        "  - y: A PyTorch tensor f shape (N,) giving training labels; y[i] = c means that\n",
        "    X[i] has label c, where 0 <= c < C.\n",
        "  - X_val: A PyTorch tensor of shape (N_val, D) giving validation data.\n",
        "  - y_val: A PyTorch tensor of shape (N_val,) giving validation labels.\n",
        "  - learning_rate: Scalar giving learning rate for optimization.\n",
        "  - learning_rate_decay: Scalar giving factor used to decay the learning rate\n",
        "    after each epoch.\n",
        "  - reg: Scalar giving regularization strength.\n",
        "  - num_iters: Number of steps to take when optimizing.\n",
        "  - batch_size: Number of training examples to use per step.\n",
        "  - verbose: boolean; if true print progress during optimization.\n",
        "\n",
        "  Returns: A dictionary giving statistics about the training process\n",
        "  \"\"\"\n",
        "  num_train = X.shape[0]\n",
        "  iterations_per_epoch = max(num_train // batch_size, 1)\n",
        "\n",
        "  # Use SGD to optimize the parameters in self.model\n",
        "  loss_history = []\n",
        "  train_acc_history = []\n",
        "  val_acc_history = []\n",
        "\n",
        "  for it in range(num_iters):\n",
        "    indices = torch.randint(num_train, (batch_size,))\n",
        "    y_batch = y[indices]\n",
        "    X_batch = X[indices]\n",
        "\n",
        "    # Compute loss and gradients using the current minibatch\n",
        "    loss, grads = loss_func(params, X_batch, y=y_batch, reg=reg)\n",
        "    loss_history.append(loss.item())\n",
        "\n",
        "    #########################################################################\n",
        "    # TODO: Use the gradients in the grads dictionary to update the         #\n",
        "    # parameters of the network (stored in the dictionary self.params)      #\n",
        "    # using stochastic gradient descent. You'll need to use the gradients   #\n",
        "    # stored in the grads dictionary defined above.                         #\n",
        "    #########################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    for param_name in params:\n",
        "        params[param_name] -= learning_rate * grads[param_name]\n",
        "    #########################################################################\n",
        "    #                             END OF YOUR CODE                          #\n",
        "    #########################################################################\n",
        "\n",
        "    if verbose and it % 100 == 0:\n",
        "      print('iteration %d / %d: loss %f' % (it, num_iters, loss.item()))\n",
        "\n",
        "    # Every epoch, check train and val accuracy and decay learning rate.\n",
        "    if it % iterations_per_epoch == 0:\n",
        "      # Check accuracy\n",
        "      y_train_pred = pred_func(params, loss_func, X_batch)\n",
        "      train_acc = (y_train_pred == y_batch).float().mean().item()\n",
        "      y_val_pred = pred_func(params, loss_func, X_val)\n",
        "      val_acc = (y_val_pred == y_val).float().mean().item()\n",
        "      train_acc_history.append(train_acc)\n",
        "      val_acc_history.append(val_acc)\n",
        "\n",
        "      # Decay learning rate\n",
        "      learning_rate *= learning_rate_decay\n",
        "\n",
        "  return {\n",
        "    'loss_history': loss_history,\n",
        "    'train_acc_history': train_acc_history,\n",
        "    'val_acc_history': val_acc_history,\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tpZM1fjuTDz"
      },
      "source": [
        "Predict function (5 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Zannb9efuTD0"
      },
      "outputs": [],
      "source": [
        "def nn_predict(params, loss_func, X):\n",
        "  \"\"\"\n",
        "  Use the trained weights of this two-layer network to predict labels for\n",
        "  data points. For each data point we predict scores for each of the C\n",
        "  classes, and assign each data point to the class with the highest score.\n",
        "\n",
        "  Inputs:\n",
        "  - params: a dictionary of PyTorch Tensor that store the weights of a model.\n",
        "    It should have following keys with shape\n",
        "        W1: First layer weights; has shape (D, H)\n",
        "        b1: First layer biases; has shape (H,)\n",
        "        W2: Second layer weights; has shape (H, C)\n",
        "        b2: Second layer biases; has shape (C,)\n",
        "  - loss_func: a loss function that computes the loss and the gradients\n",
        "  - X: A PyTorch tensor of shape (N, D) giving N D-dimensional data points to\n",
        "    classify.\n",
        "\n",
        "  Returns:\n",
        "  - y_pred: A PyTorch tensor of shape (N,) giving predicted labels for each of\n",
        "    the elements of X. For all i, y_pred[i] = c means that X[i] is predicted\n",
        "    to have class c, where 0 <= c < C.\n",
        "  \"\"\"\n",
        "  y_pred = None\n",
        "\n",
        "  ###########################################################################\n",
        "  # TODO: Implement this function; it should be VERY simple!                #\n",
        "  ###########################################################################\n",
        "  # Replace \"pass\" statement with your code\n",
        "  # Get scores from the forward pass by calling the loss function with y=None\n",
        "  scores = loss_func(params, X)\n",
        "\n",
        "  # Get predictions by taking argmax of scores\n",
        "  y_pred = torch.argmax(scores, dim=1)\n",
        "  ###########################################################################\n",
        "  #                              END OF YOUR CODE                           #\n",
        "  ###########################################################################\n",
        "\n",
        "  return y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "mbb0DY-SuTD0"
      },
      "outputs": [],
      "source": [
        "def visualization(stats):\n",
        "    print('Final training loss: ', stats['loss_history'][-1])\n",
        "\n",
        "    # plot the loss history\n",
        "    plt.plot(stats['loss_history'])\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('training loss')\n",
        "    plt.title('Training Loss history')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot the loss function and train / validation accuracies\n",
        "    plt.plot(stats['train_acc_history'], 'o', label='train')\n",
        "    plt.plot(stats['val_acc_history'], 'o', label='val')\n",
        "    plt.title('Classification accuracy history')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Clasification accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHhBY5VduTD0"
      },
      "source": [
        "Now, we can test our implementation of the neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 962
        },
        "id": "69UXmtUnuTD0",
        "outputId": "076d9b8e-508f-4ae4-a860-794a5a6a350f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "That took 93.635873s\n",
            "Final training loss:  2.3030202388763428\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "model = TwoLayerNet(input_size=X_train.shape[1], hidden_size=128, output_size=10, device='cpu')\n",
        "tic = time.time()\n",
        "stats = model.train(X_train, y_train, X_val, y_val, verbose=False, num_iters=10000)\n",
        "toc = time.time()\n",
        "print('That took %fs' % (toc - tic))\n",
        "visualization(stats)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEgJF7hSuTD0"
      },
      "source": [
        "### Hyperparameters tuning (10 pts)\n",
        "\n",
        "**Tuning**. Tuning the hyperparameters and developing intuition for how they affect the final performance is a large part of using Neural Networks, so we want you to get a lot of practice. Below, you should experiment with different values of the various hyperparameters, including hidden layer size, learning rate, and regularization strength. You might also consider tuning other parameters such as num_iters as well.\n",
        "\n",
        "**Approximate results**. To get full credit for the assignment, you should achieve a classification accuracy above 50% on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MK__DhIOL3Ti",
        "outputId": "c75688ba-ac35-4133-aba1-5da2c581614f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting hyperparameter tuning...\n",
            "Training with lr=0.5, reg=0, hidden_dim=384\n",
            "New best model: val_accuracy = 0.4480\n",
            "Training with lr=0.5, reg=0, hidden_dim=512\n",
            "Training with lr=0.5, reg=0, hidden_dim=768\n",
            "Training with lr=0.5, reg=0, hidden_dim=1024\n",
            "Training with lr=0.5, reg=1e-07, hidden_dim=384\n",
            "Training with lr=0.5, reg=1e-07, hidden_dim=512\n",
            "Training with lr=0.5, reg=1e-07, hidden_dim=768\n",
            "Training with lr=0.5, reg=1e-07, hidden_dim=1024\n",
            "Training with lr=0.5, reg=1e-06, hidden_dim=384\n",
            "New best model: val_accuracy = 0.4510\n",
            "Training with lr=0.5, reg=1e-06, hidden_dim=512\n",
            "Training with lr=0.5, reg=1e-06, hidden_dim=768\n",
            "Training with lr=0.5, reg=1e-06, hidden_dim=1024\n",
            "Training with lr=0.5, reg=1e-05, hidden_dim=384\n",
            "New best model: val_accuracy = 0.4530\n",
            "Training with lr=0.5, reg=1e-05, hidden_dim=512\n",
            "Training with lr=0.5, reg=1e-05, hidden_dim=768\n",
            "Training with lr=0.5, reg=1e-05, hidden_dim=1024\n",
            "Training with lr=0.7, reg=0, hidden_dim=384\n",
            "New best model: val_accuracy = 0.4720\n",
            "Training with lr=0.7, reg=0, hidden_dim=512\n",
            "Training with lr=0.7, reg=0, hidden_dim=768\n",
            "Training with lr=0.7, reg=0, hidden_dim=1024\n",
            "Training with lr=0.7, reg=1e-07, hidden_dim=384\n",
            "Training with lr=0.7, reg=1e-07, hidden_dim=512\n",
            "Training with lr=0.7, reg=1e-07, hidden_dim=768\n",
            "Training with lr=0.7, reg=1e-07, hidden_dim=1024\n",
            "Training with lr=0.7, reg=1e-06, hidden_dim=384\n",
            "Training with lr=0.7, reg=1e-06, hidden_dim=512\n",
            "Training with lr=0.7, reg=1e-06, hidden_dim=768\n",
            "Training with lr=0.7, reg=1e-06, hidden_dim=1024\n",
            "Training with lr=0.7, reg=1e-05, hidden_dim=384\n",
            "Training with lr=0.7, reg=1e-05, hidden_dim=512\n",
            "Training with lr=0.7, reg=1e-05, hidden_dim=768\n",
            "Training with lr=0.7, reg=1e-05, hidden_dim=1024\n",
            "Training with lr=1.0, reg=0, hidden_dim=384\n",
            "New best model: val_accuracy = 0.4960\n",
            "Training with lr=1.0, reg=0, hidden_dim=512\n",
            "Training with lr=1.0, reg=0, hidden_dim=768\n",
            "Training with lr=1.0, reg=0, hidden_dim=1024\n",
            "Training with lr=1.0, reg=1e-07, hidden_dim=384\n",
            "New best model: val_accuracy = 0.4980\n",
            "Training with lr=1.0, reg=1e-07, hidden_dim=512\n",
            "Training with lr=1.0, reg=1e-07, hidden_dim=768\n",
            "Training with lr=1.0, reg=1e-07, hidden_dim=1024\n",
            "Training with lr=1.0, reg=1e-06, hidden_dim=384\n",
            "Training with lr=1.0, reg=1e-06, hidden_dim=512\n",
            "New best model: val_accuracy = 0.5000\n",
            "Training with lr=1.0, reg=1e-06, hidden_dim=768\n",
            "Training with lr=1.0, reg=1e-06, hidden_dim=1024\n",
            "Training with lr=1.0, reg=1e-05, hidden_dim=384\n",
            "Training with lr=1.0, reg=1e-05, hidden_dim=512\n",
            "New best model: val_accuracy = 0.5010\n",
            "Training with lr=1.0, reg=1e-05, hidden_dim=768\n",
            "Training with lr=1.0, reg=1e-05, hidden_dim=1024\n",
            "Best hyperparameters: lr=1.0, reg=1e-05, hidden_dim=512\n",
            "iteration 0 / 8000: loss 2.302712\n",
            "iteration 100 / 8000: loss 2.277854\n",
            "iteration 200 / 8000: loss 2.040915\n",
            "iteration 300 / 8000: loss 1.831343\n",
            "iteration 400 / 8000: loss 1.776872\n",
            "iteration 500 / 8000: loss 1.749563\n",
            "iteration 600 / 8000: loss 1.661909\n",
            "iteration 700 / 8000: loss 1.730476\n",
            "iteration 800 / 8000: loss 1.656007\n",
            "iteration 900 / 8000: loss 1.657022\n",
            "iteration 1000 / 8000: loss 1.531593\n",
            "iteration 1100 / 8000: loss 1.532537\n",
            "iteration 1200 / 8000: loss 1.407128\n",
            "iteration 1300 / 8000: loss 1.429646\n",
            "iteration 1400 / 8000: loss 1.504884\n",
            "iteration 1500 / 8000: loss 1.379798\n",
            "iteration 1600 / 8000: loss 1.300159\n",
            "iteration 1700 / 8000: loss 1.406824\n",
            "iteration 1800 / 8000: loss 1.456739\n",
            "iteration 1900 / 8000: loss 1.409551\n",
            "iteration 2000 / 8000: loss 1.182867\n",
            "iteration 2100 / 8000: loss 1.412113\n",
            "iteration 2200 / 8000: loss 1.426808\n",
            "iteration 2300 / 8000: loss 1.400980\n",
            "iteration 2400 / 8000: loss 1.241284\n",
            "iteration 2500 / 8000: loss 1.304014\n",
            "iteration 2600 / 8000: loss 1.343309\n",
            "iteration 2700 / 8000: loss 1.239685\n",
            "iteration 2800 / 8000: loss 1.265587\n",
            "iteration 2900 / 8000: loss 1.346947\n",
            "iteration 3000 / 8000: loss 1.339279\n",
            "iteration 3100 / 8000: loss 1.307005\n",
            "iteration 3200 / 8000: loss 1.227780\n",
            "iteration 3300 / 8000: loss 1.109372\n",
            "iteration 3400 / 8000: loss 1.135645\n",
            "iteration 3500 / 8000: loss 1.216051\n",
            "iteration 3600 / 8000: loss 1.245631\n",
            "iteration 3700 / 8000: loss 1.210782\n",
            "iteration 3800 / 8000: loss 1.275728\n",
            "iteration 3900 / 8000: loss 1.172280\n",
            "iteration 4000 / 8000: loss 1.267400\n",
            "iteration 4100 / 8000: loss 1.219921\n",
            "iteration 4200 / 8000: loss 1.059330\n",
            "iteration 4300 / 8000: loss 1.167317\n",
            "iteration 4400 / 8000: loss 1.088710\n",
            "iteration 4500 / 8000: loss 1.123137\n",
            "iteration 4600 / 8000: loss 1.114026\n",
            "iteration 4700 / 8000: loss 1.196509\n",
            "iteration 4800 / 8000: loss 1.119819\n",
            "iteration 4900 / 8000: loss 1.127917\n",
            "iteration 5000 / 8000: loss 1.200658\n",
            "iteration 5100 / 8000: loss 1.325636\n",
            "iteration 5200 / 8000: loss 1.216488\n",
            "iteration 5300 / 8000: loss 1.191004\n",
            "iteration 5400 / 8000: loss 1.154697\n",
            "iteration 5500 / 8000: loss 1.206100\n",
            "iteration 5600 / 8000: loss 1.127197\n",
            "iteration 5700 / 8000: loss 1.189977\n",
            "iteration 5800 / 8000: loss 1.152224\n",
            "iteration 5900 / 8000: loss 1.077864\n",
            "iteration 6000 / 8000: loss 1.179621\n",
            "iteration 6100 / 8000: loss 1.119994\n",
            "iteration 6200 / 8000: loss 1.270168\n",
            "iteration 6300 / 8000: loss 1.172428\n",
            "iteration 6400 / 8000: loss 1.135115\n",
            "iteration 6500 / 8000: loss 1.098096\n",
            "iteration 6600 / 8000: loss 1.137819\n",
            "iteration 6700 / 8000: loss 1.021511\n",
            "iteration 6800 / 8000: loss 1.260520\n",
            "iteration 6900 / 8000: loss 1.162058\n",
            "iteration 7000 / 8000: loss 1.167908\n",
            "iteration 7100 / 8000: loss 1.041457\n",
            "iteration 7200 / 8000: loss 1.154747\n",
            "iteration 7300 / 8000: loss 1.195556\n",
            "iteration 7400 / 8000: loss 1.142495\n",
            "iteration 7500 / 8000: loss 1.161962\n",
            "iteration 7600 / 8000: loss 1.124055\n",
            "iteration 7700 / 8000: loss 1.012036\n",
            "iteration 7800 / 8000: loss 1.132277\n",
            "iteration 7900 / 8000: loss 1.185427\n",
            "lr 5.000000e-01 reg 0.000000e+00 H 3.840000e+02 train accuracy: 0.450000 val accuracy: 0.448000\n",
            "lr 5.000000e-01 reg 0.000000e+00 H 5.120000e+02 train accuracy: 0.515000 val accuracy: 0.441000\n",
            "lr 5.000000e-01 reg 0.000000e+00 H 7.680000e+02 train accuracy: 0.505000 val accuracy: 0.434000\n",
            "lr 5.000000e-01 reg 0.000000e+00 H 1.024000e+03 train accuracy: 0.485000 val accuracy: 0.438000\n",
            "lr 5.000000e-01 reg 1.000000e-07 H 3.840000e+02 train accuracy: 0.460000 val accuracy: 0.447000\n",
            "lr 5.000000e-01 reg 1.000000e-07 H 5.120000e+02 train accuracy: 0.500000 val accuracy: 0.445000\n",
            "lr 5.000000e-01 reg 1.000000e-07 H 7.680000e+02 train accuracy: 0.505000 val accuracy: 0.430000\n",
            "lr 5.000000e-01 reg 1.000000e-07 H 1.024000e+03 train accuracy: 0.495000 val accuracy: 0.440000\n",
            "lr 5.000000e-01 reg 1.000000e-06 H 3.840000e+02 train accuracy: 0.460000 val accuracy: 0.451000\n",
            "lr 5.000000e-01 reg 1.000000e-06 H 5.120000e+02 train accuracy: 0.505000 val accuracy: 0.441000\n",
            "lr 5.000000e-01 reg 1.000000e-06 H 7.680000e+02 train accuracy: 0.495000 val accuracy: 0.434000\n",
            "lr 5.000000e-01 reg 1.000000e-06 H 1.024000e+03 train accuracy: 0.490000 val accuracy: 0.447000\n",
            "lr 5.000000e-01 reg 1.000000e-05 H 3.840000e+02 train accuracy: 0.470000 val accuracy: 0.453000\n",
            "lr 5.000000e-01 reg 1.000000e-05 H 5.120000e+02 train accuracy: 0.520000 val accuracy: 0.442000\n",
            "lr 5.000000e-01 reg 1.000000e-05 H 7.680000e+02 train accuracy: 0.505000 val accuracy: 0.427000\n",
            "lr 5.000000e-01 reg 1.000000e-05 H 1.024000e+03 train accuracy: 0.490000 val accuracy: 0.442000\n",
            "lr 7.000000e-01 reg 0.000000e+00 H 3.840000e+02 train accuracy: 0.495000 val accuracy: 0.472000\n",
            "lr 7.000000e-01 reg 0.000000e+00 H 5.120000e+02 train accuracy: 0.560000 val accuracy: 0.460000\n",
            "lr 7.000000e-01 reg 0.000000e+00 H 7.680000e+02 train accuracy: 0.555000 val accuracy: 0.450000\n",
            "lr 7.000000e-01 reg 0.000000e+00 H 1.024000e+03 train accuracy: 0.525000 val accuracy: 0.460000\n",
            "lr 7.000000e-01 reg 1.000000e-07 H 3.840000e+02 train accuracy: 0.490000 val accuracy: 0.469000\n",
            "lr 7.000000e-01 reg 1.000000e-07 H 5.120000e+02 train accuracy: 0.540000 val accuracy: 0.467000\n",
            "lr 7.000000e-01 reg 1.000000e-07 H 7.680000e+02 train accuracy: 0.560000 val accuracy: 0.447000\n",
            "lr 7.000000e-01 reg 1.000000e-07 H 1.024000e+03 train accuracy: 0.530000 val accuracy: 0.468000\n",
            "lr 7.000000e-01 reg 1.000000e-06 H 3.840000e+02 train accuracy: 0.475000 val accuracy: 0.470000\n",
            "lr 7.000000e-01 reg 1.000000e-06 H 5.120000e+02 train accuracy: 0.560000 val accuracy: 0.457000\n",
            "lr 7.000000e-01 reg 1.000000e-06 H 7.680000e+02 train accuracy: 0.545000 val accuracy: 0.453000\n",
            "lr 7.000000e-01 reg 1.000000e-06 H 1.024000e+03 train accuracy: 0.520000 val accuracy: 0.459000\n",
            "lr 7.000000e-01 reg 1.000000e-05 H 3.840000e+02 train accuracy: 0.470000 val accuracy: 0.464000\n",
            "lr 7.000000e-01 reg 1.000000e-05 H 5.120000e+02 train accuracy: 0.550000 val accuracy: 0.463000\n",
            "lr 7.000000e-01 reg 1.000000e-05 H 7.680000e+02 train accuracy: 0.550000 val accuracy: 0.451000\n",
            "lr 7.000000e-01 reg 1.000000e-05 H 1.024000e+03 train accuracy: 0.515000 val accuracy: 0.462000\n",
            "lr 1.000000e+00 reg 0.000000e+00 H 3.840000e+02 train accuracy: 0.530000 val accuracy: 0.496000\n",
            "lr 1.000000e+00 reg 0.000000e+00 H 5.120000e+02 train accuracy: 0.580000 val accuracy: 0.493000\n",
            "lr 1.000000e+00 reg 0.000000e+00 H 7.680000e+02 train accuracy: 0.600000 val accuracy: 0.484000\n",
            "lr 1.000000e+00 reg 0.000000e+00 H 1.024000e+03 train accuracy: 0.560000 val accuracy: 0.468000\n",
            "lr 1.000000e+00 reg 1.000000e-07 H 3.840000e+02 train accuracy: 0.510000 val accuracy: 0.498000\n",
            "lr 1.000000e+00 reg 1.000000e-07 H 5.120000e+02 train accuracy: 0.585000 val accuracy: 0.495000\n",
            "lr 1.000000e+00 reg 1.000000e-07 H 7.680000e+02 train accuracy: 0.595000 val accuracy: 0.486000\n",
            "lr 1.000000e+00 reg 1.000000e-07 H 1.024000e+03 train accuracy: 0.550000 val accuracy: 0.473000\n",
            "lr 1.000000e+00 reg 1.000000e-06 H 3.840000e+02 train accuracy: 0.510000 val accuracy: 0.494000\n",
            "lr 1.000000e+00 reg 1.000000e-06 H 5.120000e+02 train accuracy: 0.585000 val accuracy: 0.500000\n",
            "lr 1.000000e+00 reg 1.000000e-06 H 7.680000e+02 train accuracy: 0.575000 val accuracy: 0.477000\n",
            "lr 1.000000e+00 reg 1.000000e-06 H 1.024000e+03 train accuracy: 0.555000 val accuracy: 0.473000\n",
            "lr 1.000000e+00 reg 1.000000e-05 H 3.840000e+02 train accuracy: 0.520000 val accuracy: 0.490000\n",
            "lr 1.000000e+00 reg 1.000000e-05 H 5.120000e+02 train accuracy: 0.575000 val accuracy: 0.501000\n",
            "lr 1.000000e+00 reg 1.000000e-05 H 7.680000e+02 train accuracy: 0.600000 val accuracy: 0.484000\n",
            "lr 1.000000e+00 reg 1.000000e-05 H 1.024000e+03 train accuracy: 0.575000 val accuracy: 0.483000\n",
            "best validation accuracy achieved: 0.501000\n",
            "final test accuracy 2-layered neural network achieved: 0.544000\n"
          ]
        }
      ],
      "source": [
        "results = {}\n",
        "best_val = -1\n",
        "best_nn = None\n",
        "################################################################################\n",
        "# TODO (10 pts):                                                               #\n",
        "# Use the validation set to set the learning rate and regularization strength. #\n",
        "# This should be identical to the validation that you did for the SVM; save    #\n",
        "# the best trained softmax classifer in best_softmax.                          #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "# Extended hyperparameter search space\n",
        "learning_rates = [0.5, 0.7, 1.0]  # Try even higher learning rates\n",
        "regularization_strengths = [0, 1e-7, 1e-6, 1e-5]  # Try lower regularization\n",
        "hidden_dims = [384, 512, 768, 1024]  # Try even larger hidden layers\n",
        "# Try combinations of hyperparameters\n",
        "print(\"Starting hyperparameter tuning...\")\n",
        "for lr in learning_rates:\n",
        "    for reg in regularization_strengths:\n",
        "        for hidden_dim in hidden_dims:\n",
        "            print(f\"Training with lr={lr}, reg={reg}, hidden_dim={hidden_dim}\")\n",
        "\n",
        "            # Create a new network with current hyperparameters\n",
        "            model = TwoLayerNet(\n",
        "                input_size=X_train.shape[1],\n",
        "                hidden_size=hidden_dim,\n",
        "                output_size=10,\n",
        "                device='cpu'\n",
        "            )\n",
        "\n",
        "            # Train with current hyperparameters (use fewer iterations for faster tuning)\n",
        "            # Apply more aggressive learning rate decay for higher learning rates\n",
        "            lr_decay = 0.9 if lr >= 0.3 else 0.95  # More aggressive decay for higher learning rates\n",
        "\n",
        "            stats = model.train(\n",
        "                X_train, y_train, X_val, y_val,\n",
        "                learning_rate=lr,\n",
        "                learning_rate_decay=lr_decay,  # Apply more aggressive LR decay\n",
        "                reg=reg,\n",
        "                num_iters=2000,  # Reduced number of iterations for faster tuning\n",
        "                batch_size=200,\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "            # Get validation accuracy (take the last epoch's accuracy)\n",
        "            train_accuracy = stats['train_acc_history'][-1]\n",
        "            val_accuracy = stats['val_acc_history'][-1]\n",
        "\n",
        "            # Store results\n",
        "            results[(lr, reg, hidden_dim)] = (train_accuracy, val_accuracy)\n",
        "\n",
        "            # Update best validation accuracy and model\n",
        "            if val_accuracy > best_val:\n",
        "                best_val = val_accuracy\n",
        "                best_nn = model\n",
        "                print(f\"New best model: val_accuracy = {val_accuracy:.4f}\")\n",
        "\n",
        "# After finding the best hyperparameters, retrain with more iterations\n",
        "if best_nn is not None:\n",
        "    best_lr, best_reg, best_hidden = max(results.keys(), key=lambda k: results[k][1])\n",
        "    print(f\"Best hyperparameters: lr={best_lr}, reg={best_reg}, hidden_dim={best_hidden}\")\n",
        "\n",
        "    # Set appropriate learning rate decay based on the learning rate\n",
        "    best_lr_decay = 0.9 if best_lr >= 0.3 else 0.95\n",
        "\n",
        "    # Retrain the best model with more iterations\n",
        "    best_nn = TwoLayerNet(\n",
        "        input_size=X_train.shape[1],\n",
        "        hidden_size=best_hidden,\n",
        "        output_size=10,\n",
        "        device='cpu'\n",
        "    )\n",
        "\n",
        "    # Longer training period (8000-10000 iterations)\n",
        "    best_stats = best_nn.train(\n",
        "        X_train, y_train, X_val, y_val,\n",
        "        learning_rate=best_lr,\n",
        "        learning_rate_decay=best_lr_decay,\n",
        "        reg=best_reg,\n",
        "        num_iters=8000,  # Increased iterations for final model\n",
        "        batch_size=200,\n",
        "        verbose=True\n",
        "    )\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "# Print out results.\n",
        "for lr, reg, H in sorted(results):\n",
        "    train_accuracy, val_accuracy = results[(lr, reg, H)]\n",
        "    print('lr %e reg %e H %e train accuracy: %f val accuracy: %f' % (\n",
        "                lr, reg, H, train_accuracy, val_accuracy))\n",
        "\n",
        "print('best validation accuracy achieved: %f' % best_val)\n",
        "\n",
        "y_test_pred = best_nn.predict(X_test)\n",
        "test_acc = (y_test_pred == y_test).double().mean().item()\n",
        "print('final test accuracy 2-layered neural network achieved: %f' % test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkodX4KYRcEA"
      },
      "source": [
        "# Acknowledgement\n",
        "\n",
        "Credits to [UMichigan's 498/598 Deep Learning for Computer Vision](https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2020/) and Stanfords's [CS231n: Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/), some code is adapted from their courses's assignments."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Sp9dU9BxtSsd"
      ],
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}